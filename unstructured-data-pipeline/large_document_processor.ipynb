{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Large Document Splitter\n",
        "\n",
        "This notebook splits large documents (>100MB or >500 pages) that exceed the limits of Snowflake's `AI_PARSE_DOCUMENT` function.\n",
        "\n",
        "## Process Flow:\n",
        "1. Query the `large_document_registry` table for pending documents\n",
        "2. Download each large document from the external stage\n",
        "3. Split PDFs into smaller parts (max 400 pages each)\n",
        "4. **Upload split parts back to the SAME external stage**\n",
        "5. The existing event-driven pipeline automatically processes the split parts\n",
        "\n",
        "## How It Works:\n",
        "- Split parts uploaded to S3 trigger the directory table auto-refresh\n",
        "- The stream captures the new files\n",
        "- Tasks automatically run: Parse ‚Üí Classify ‚Üí Extract ‚Üí Chunk\n",
        "- No manual intervention needed after splitting!\n",
        "\n",
        "## Requirements:\n",
        "- Snowflake Container Runtime with Python 3.10+\n",
        "- pypdf package for PDF splitting\n",
        "- Write access to the external S3 stage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 1: Import Libraries and Initialize Session\n",
        "# ============================================\n",
        "\n",
        "import io\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# Snowpark imports\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.functions import col, lit, current_timestamp\n",
        "from snowflake.snowpark.types import StructType, StructField, StringType, IntegerType, BinaryType\n",
        "\n",
        "# PDF processing\n",
        "try:\n",
        "    from pypdf import PdfReader, PdfWriter\n",
        "    print(\"‚úÖ pypdf imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è pypdf not found, installing...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'pypdf'])\n",
        "    from pypdf import PdfReader, PdfWriter\n",
        "    print(\"‚úÖ pypdf installed and imported\")\n",
        "\n",
        "# Get active Snowflake session (Container Runtime)\n",
        "session = get_active_session()\n",
        "print(f\"‚úÖ Connected to Snowflake\")\n",
        "print(f\"   Current Database: {session.get_current_database()}\")\n",
        "print(f\"   Current Schema: {session.get_current_schema()}\")\n",
        "print(f\"   Current Warehouse: {session.get_current_warehouse()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 2: Configuration Constants\n",
        "# ============================================\n",
        "\n",
        "# Processing limits (stay under AI_PARSE_DOCUMENT limits)\n",
        "MAX_PAGES_PER_PART = 400  # AI_PARSE_DOCUMENT limit is 500 pages\n",
        "MAX_SIZE_BYTES = 100 * 1024 * 1024  # 100MB\n",
        "MAX_SIZE_PER_PART_MB = 90  # Stay under 100MB limit per part\n",
        "\n",
        "# Database objects\n",
        "DATABASE = \"document_db\"\n",
        "SCHEMA = \"s3_documents\"\n",
        "EXTERNAL_STAGE = f\"@{DATABASE}.{SCHEMA}.document_stage\"\n",
        "\n",
        "# Tables\n",
        "LARGE_DOC_REGISTRY = f\"{DATABASE}.{SCHEMA}.large_document_registry\"\n",
        "SPLIT_PARTS_TABLE = f\"{DATABASE}.{SCHEMA}.document_split_parts\"\n",
        "\n",
        "# Folder for split documents (within the same external stage)\n",
        "SPLIT_FOLDER = \"split_documents\"\n",
        "\n",
        "print(\"üìã Configuration:\")\n",
        "print(f\"   Max pages per part: {MAX_PAGES_PER_PART}\")\n",
        "print(f\"   Max size per part: {MAX_SIZE_PER_PART_MB}MB\")\n",
        "print(f\"   External stage: {EXTERNAL_STAGE}\")\n",
        "print(f\"   Split folder: {SPLIT_FOLDER}/\")\n",
        "print(f\"\")\n",
        "print(\"üí° Split parts will be uploaded to the SAME external stage\")\n",
        "print(\"   The event-driven pipeline will automatically process them!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 3: Helper Functions - PDF Operations\n",
        "# ============================================\n",
        "\n",
        "def get_pdf_info(pdf_bytes: bytes) -> Dict:\n",
        "    \"\"\"\n",
        "    Get information about a PDF file.\n",
        "    \n",
        "    Args:\n",
        "        pdf_bytes: Binary content of the PDF\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with page count, size, and metadata\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pdf_stream = io.BytesIO(pdf_bytes)\n",
        "        reader = PdfReader(pdf_stream)\n",
        "        \n",
        "        return {\n",
        "            'page_count': len(reader.pages),\n",
        "            'size_bytes': len(pdf_bytes),\n",
        "            'size_mb': round(len(pdf_bytes) / (1024 * 1024), 2),\n",
        "            'is_encrypted': reader.is_encrypted,\n",
        "            'metadata': dict(reader.metadata) if reader.metadata else {}\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "\n",
        "def split_pdf(pdf_bytes: bytes, max_pages: int = MAX_PAGES_PER_PART) -> List[Tuple[bytes, Dict]]:\n",
        "    \"\"\"\n",
        "    Split a PDF into smaller parts.\n",
        "    \n",
        "    Args:\n",
        "        pdf_bytes: Binary content of the PDF\n",
        "        max_pages: Maximum pages per split part\n",
        "    \n",
        "    Returns:\n",
        "        List of tuples containing (part_bytes, part_metadata)\n",
        "    \"\"\"\n",
        "    pdf_stream = io.BytesIO(pdf_bytes)\n",
        "    reader = PdfReader(pdf_stream)\n",
        "    total_pages = len(reader.pages)\n",
        "    \n",
        "    # Calculate number of parts needed\n",
        "    num_parts = (total_pages + max_pages - 1) // max_pages\n",
        "    \n",
        "    parts = []\n",
        "    \n",
        "    for part_num in range(num_parts):\n",
        "        start_page = part_num * max_pages\n",
        "        end_page = min((part_num + 1) * max_pages, total_pages)\n",
        "        \n",
        "        # Create a new PDF writer for this part\n",
        "        writer = PdfWriter()\n",
        "        \n",
        "        for page_idx in range(start_page, end_page):\n",
        "            writer.add_page(reader.pages[page_idx])\n",
        "        \n",
        "        # Write part to bytes\n",
        "        output_stream = io.BytesIO()\n",
        "        writer.write(output_stream)\n",
        "        part_bytes = output_stream.getvalue()\n",
        "        \n",
        "        part_metadata = {\n",
        "            'part_number': part_num + 1,\n",
        "            'total_parts': num_parts,\n",
        "            'page_start': start_page + 1,\n",
        "            'page_end': end_page,\n",
        "            'total_pages': total_pages,\n",
        "            'part_size_bytes': len(part_bytes),\n",
        "            'part_size_mb': round(len(part_bytes) / (1024 * 1024), 2)\n",
        "        }\n",
        "        \n",
        "        parts.append((part_bytes, part_metadata))\n",
        "        print(f\"   Created part {part_num + 1}/{num_parts}: pages {start_page + 1}-{end_page}, size: {part_metadata['part_size_mb']}MB\")\n",
        "    \n",
        "    return parts\n",
        "\n",
        "print(\"‚úÖ PDF helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 4: Helper Functions - Snowflake Stage Operations\n",
        "# ============================================\n",
        "\n",
        "def download_file_from_stage(file_path: str) -> bytes:\n",
        "    \"\"\"\n",
        "    Download a file from the external stage.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Relative path to the file in the stage\n",
        "    \n",
        "    Returns:\n",
        "        Binary content of the file\n",
        "    \"\"\"\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        # Use GET command to download file\n",
        "        get_query = f\"GET {EXTERNAL_STAGE}/{file_path} file://{tmp_dir}/\"\n",
        "        session.sql(get_query).collect()\n",
        "        \n",
        "        # Read the downloaded file\n",
        "        file_name = os.path.basename(file_path)\n",
        "        local_path = os.path.join(tmp_dir, file_name)\n",
        "        \n",
        "        # Handle potential .gz compression from GET\n",
        "        if os.path.exists(local_path + '.gz'):\n",
        "            import gzip\n",
        "            with gzip.open(local_path + '.gz', 'rb') as f:\n",
        "                return f.read()\n",
        "        elif os.path.exists(local_path):\n",
        "            with open(local_path, 'rb') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Downloaded file not found: {local_path}\")\n",
        "\n",
        "\n",
        "def upload_part_to_external_stage(part_bytes: bytes, part_name: str, subfolder: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Upload a split part back to the SAME external stage.\n",
        "    The event-driven pipeline will automatically pick it up!\n",
        "    \n",
        "    Args:\n",
        "        part_bytes: Binary content of the part\n",
        "        part_name: Name for the uploaded file\n",
        "        subfolder: Optional subfolder within the stage\n",
        "    \n",
        "    Returns:\n",
        "        Stage path of the uploaded file\n",
        "    \"\"\"\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        # Write bytes to temp file\n",
        "        local_path = os.path.join(tmp_dir, part_name)\n",
        "        with open(local_path, 'wb') as f:\n",
        "            f.write(part_bytes)\n",
        "        \n",
        "        # Build upload path - same external stage, optional subfolder\n",
        "        if subfolder:\n",
        "            upload_path = f\"{EXTERNAL_STAGE}/{subfolder}/\"\n",
        "        else:\n",
        "            upload_path = f\"{EXTERNAL_STAGE}/{SPLIT_FOLDER}/\"\n",
        "        \n",
        "        # Upload to external stage (AUTO_COMPRESS=FALSE to keep PDF intact)\n",
        "        put_query = f\"PUT file://{local_path} {upload_path} AUTO_COMPRESS=FALSE OVERWRITE=TRUE\"\n",
        "        session.sql(put_query).collect()\n",
        "        \n",
        "        stage_path = f\"{upload_path}{part_name}\"\n",
        "        return stage_path\n",
        "\n",
        "\n",
        "def remove_original_large_file(file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Optionally remove the original large file after successful splitting.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Relative path to the file in the stage\n",
        "    \n",
        "    Returns:\n",
        "        True if removed successfully, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        remove_query = f\"REMOVE {EXTERNAL_STAGE}/{file_path}\"\n",
        "        session.sql(remove_query).collect()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Could not remove original file: {str(e)[:50]}\")\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ Snowflake stage helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 5: Query Pending Large Documents\n",
        "# ============================================\n",
        "\n",
        "def get_pending_large_documents() -> list:\n",
        "    \"\"\"\n",
        "    Query the large_document_registry for documents pending processing.\n",
        "    \n",
        "    Returns:\n",
        "        List of pending document records\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "    SELECT \n",
        "        registry_id,\n",
        "        original_file_path,\n",
        "        original_file_name,\n",
        "        original_file_size,\n",
        "        estimated_pages,\n",
        "        split_status,\n",
        "        created_timestamp\n",
        "    FROM {LARGE_DOC_REGISTRY}\n",
        "    WHERE split_status IN ('pending', 'pending_split', 'requires_external_processing')\n",
        "      AND split_required = TRUE\n",
        "    ORDER BY created_timestamp ASC\n",
        "    \"\"\"\n",
        "    \n",
        "    results = session.sql(query).collect()\n",
        "    return results\n",
        "\n",
        "# Get pending documents\n",
        "pending_docs = get_pending_large_documents()\n",
        "\n",
        "print(f\"\\nüìã Found {len(pending_docs)} large documents pending processing:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for doc in pending_docs:\n",
        "    size_mb = round(doc['ORIGINAL_FILE_SIZE'] / (1024 * 1024), 2) if doc['ORIGINAL_FILE_SIZE'] else 'N/A'\n",
        "    print(f\"  ‚Ä¢ {doc['ORIGINAL_FILE_NAME']}\")\n",
        "    print(f\"    Registry ID: {doc['REGISTRY_ID']}\")\n",
        "    print(f\"    Size: {size_mb} MB\")\n",
        "    print(f\"    Status: {doc['SPLIT_STATUS']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 7: Main Processing Function - Split & Upload\n",
        "# ============================================\n",
        "\n",
        "# Configuration: Whether to remove original large file after splitting\n",
        "REMOVE_ORIGINAL_AFTER_SPLIT = False  # Set to True to delete originals\n",
        "\n",
        "def split_large_document(doc_record: dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Split a large document and upload parts back to the external stage.\n",
        "    The event-driven pipeline will automatically process the split parts!\n",
        "    \n",
        "    Args:\n",
        "        doc_record: Document record from the registry\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with processing results\n",
        "    \"\"\"\n",
        "    registry_id = doc_record['REGISTRY_ID']\n",
        "    file_path = doc_record['ORIGINAL_FILE_PATH']\n",
        "    file_name = doc_record['ORIGINAL_FILE_NAME']\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Splitting: {file_name}\")\n",
        "    print(f\"Registry ID: {registry_id}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = {\n",
        "        'registry_id': registry_id,\n",
        "        'file_name': file_name,\n",
        "        'status': 'pending',\n",
        "        'parts_created': 0,\n",
        "        'parts_uploaded': 0\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Update status to processing\n",
        "        update_registry_status(registry_id, 'splitting')\n",
        "        \n",
        "        # Step 2: Download the file from external stage\n",
        "        print(f\"\\nüì• Step 1: Downloading file from stage...\")\n",
        "        pdf_bytes = download_file_from_stage(file_path)\n",
        "        print(f\"   Downloaded: {len(pdf_bytes) / (1024*1024):.2f} MB\")\n",
        "        \n",
        "        # Step 3: Get PDF info\n",
        "        print(f\"\\nüìä Step 2: Analyzing PDF...\")\n",
        "        pdf_info = get_pdf_info(pdf_bytes)\n",
        "        \n",
        "        if 'error' in pdf_info:\n",
        "            raise Exception(f\"PDF analysis failed: {pdf_info['error']}\")\n",
        "        \n",
        "        print(f\"   Pages: {pdf_info['page_count']}\")\n",
        "        print(f\"   Size: {pdf_info['size_mb']} MB\")\n",
        "        print(f\"   Encrypted: {pdf_info['is_encrypted']}\")\n",
        "        \n",
        "        # Update estimated pages\n",
        "        update_registry_status(registry_id, 'splitting', \n",
        "                               estimated_pages=pdf_info['page_count'])\n",
        "        \n",
        "        # Check if document actually needs splitting\n",
        "        needs_split = (pdf_info['page_count'] > MAX_PAGES_PER_PART or \n",
        "                       pdf_info['size_mb'] > MAX_SIZE_PER_PART_MB)\n",
        "        \n",
        "        if not needs_split:\n",
        "            print(f\"\\n‚úÖ Document is within limits - no split needed!\")\n",
        "            print(f\"   The existing pipeline can process this file directly.\")\n",
        "            update_registry_status(registry_id, 'no_split_needed', split_count=0)\n",
        "            result['status'] = 'no_split_needed'\n",
        "            return result\n",
        "        \n",
        "        # Step 4: Split PDF\n",
        "        print(f\"\\n‚úÇÔ∏è Step 3: Splitting PDF into parts...\")\n",
        "        parts = split_pdf(pdf_bytes, MAX_PAGES_PER_PART)\n",
        "        result['parts_created'] = len(parts)\n",
        "        print(f\"   Created {len(parts)} part(s)\")\n",
        "        \n",
        "        # Step 5: Upload parts back to external stage\n",
        "        print(f\"\\nüì§ Step 4: Uploading parts to external stage...\")\n",
        "        print(f\"   (Event-driven pipeline will process them automatically!)\")\n",
        "        \n",
        "        for part_bytes, part_meta in parts:\n",
        "            part_num = part_meta['part_number']\n",
        "            total_parts = part_meta['total_parts']\n",
        "            \n",
        "            # Generate part filename\n",
        "            base_name = file_name.rsplit('.', 1)[0]\n",
        "            part_name = f\"{base_name}_part_{part_num}_of_{total_parts}.pdf\"\n",
        "            \n",
        "            print(f\"\\n   Uploading part {part_num}/{total_parts}: {part_name}\")\n",
        "            \n",
        "            # Upload part to EXTERNAL stage (same as source!)\n",
        "            stage_path = upload_part_to_external_stage(part_bytes, part_name)\n",
        "            print(f\"   üì§ Uploaded to: {stage_path}\")\n",
        "            \n",
        "            # Insert part record for tracking\n",
        "            insert_split_part_record(registry_id, part_meta, stage_path)\n",
        "            result['parts_uploaded'] += 1\n",
        "        \n",
        "        # Step 6: Optionally remove the original large file\n",
        "        if REMOVE_ORIGINAL_AFTER_SPLIT:\n",
        "            print(f\"\\nüóëÔ∏è Step 5: Removing original large file...\")\n",
        "            if remove_original_large_file(file_path):\n",
        "                print(f\"   ‚úÖ Original file removed: {file_path}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è Could not remove original file\")\n",
        "        \n",
        "        # Step 7: Update registry status\n",
        "        update_registry_status(registry_id, 'split_uploaded', split_count=len(parts))\n",
        "        \n",
        "        result['status'] = 'success'\n",
        "        \n",
        "        print(f\"\\n‚úÖ Successfully split: {file_name}\")\n",
        "        print(f\"   ‚Üí {len(parts)} parts uploaded to external stage\")\n",
        "        print(f\"   ‚Üí Event-driven pipeline will process them automatically!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)[:500]\n",
        "        print(f\"\\n‚ùå Error splitting {file_name}: {error_msg}\")\n",
        "        \n",
        "        # Update registry with error status\n",
        "        update_registry_status(registry_id, f'error: {error_msg[:100]}')\n",
        "        \n",
        "        result['status'] = 'error'\n",
        "        result['error'] = error_msg\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"‚úÖ Main split function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 8: Split All Pending Large Documents\n",
        "# ============================================\n",
        "\n",
        "# Process all pending documents\n",
        "print(f\"\\n{'#'*60}\")\n",
        "print(f\"# SPLITTING LARGE DOCUMENTS\")\n",
        "print(f\"# Documents to split: {len(pending_docs)}\")\n",
        "print(f\"{'#'*60}\")\n",
        "\n",
        "splitting_results = []\n",
        "\n",
        "for i, doc in enumerate(pending_docs, 1):\n",
        "    print(f\"\\n[{i}/{len(pending_docs)}] Splitting document...\")\n",
        "    result = split_large_document(doc)\n",
        "    splitting_results.append(result)\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"SPLITTING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "success_count = sum(1 for r in splitting_results if r['status'] == 'success')\n",
        "no_split_count = sum(1 for r in splitting_results if r['status'] == 'no_split_needed')\n",
        "error_count = sum(1 for r in splitting_results if r['status'] == 'error')\n",
        "total_parts = sum(r.get('parts_uploaded', 0) for r in splitting_results)\n",
        "\n",
        "print(f\"\\nTotal Documents: {len(splitting_results)}\")\n",
        "print(f\"  ‚úÖ Split Successfully: {success_count}\")\n",
        "print(f\"  ‚è≠Ô∏è  No Split Needed: {no_split_count}\")\n",
        "print(f\"  ‚ùå Errors: {error_count}\")\n",
        "print(f\"  üìÑ Total Parts Created: {total_parts}\")\n",
        "\n",
        "if splitting_results:\n",
        "    print(f\"\\nDetails:\")\n",
        "    for result in splitting_results:\n",
        "        if result['status'] == 'success':\n",
        "            status_icon = '‚úÖ'\n",
        "        elif result['status'] == 'no_split_needed':\n",
        "            status_icon = '‚è≠Ô∏è'\n",
        "        else:\n",
        "            status_icon = '‚ùå'\n",
        "        \n",
        "        print(f\"  {status_icon} {result['file_name']}\")\n",
        "        if result['status'] == 'success':\n",
        "            print(f\"      ‚Üí {result['parts_created']} parts uploaded to external stage\")\n",
        "        elif result['status'] == 'error':\n",
        "            print(f\"      Error: {result.get('error', 'Unknown')[:80]}...\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üöÄ Event-driven pipeline will automatically process the split parts!\")\n",
        "print(f\"   Check parsed_documents table in a few minutes for results.\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 9: Check Pipeline Progress (Optional)\n",
        "# ============================================\n",
        "\n",
        "# The event-driven pipeline runs automatically!\n",
        "# This cell lets you check the progress of split parts through the pipeline.\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVENT-DRIVEN PIPELINE STATUS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüí° Split parts were uploaded to the external stage.\")\n",
        "print(\"   The following happens automatically:\")\n",
        "print(\"   1. Directory table auto-refresh detects new files\")\n",
        "print(\"   2. Stream captures the new split parts\")\n",
        "print(\"   3. parse_documents_task triggers and parses the parts\")\n",
        "print(\"   4. classify_documents_task classifies them\")\n",
        "print(\"   5. extract_documents_task extracts attributes\")\n",
        "print(\"   6. chunk_documents_task creates searchable chunks\")\n",
        "print(\"   7. Cortex Search Service updates (within 2 minutes)\")\n",
        "\n",
        "# Check if any split parts have been processed yet\n",
        "try:\n",
        "    check_query = f\"\"\"\n",
        "    SELECT \n",
        "        'Stream' as stage,\n",
        "        COUNT(*) as count\n",
        "    FROM {DATABASE}.{SCHEMA}.new_documents_stream\n",
        "    WHERE relative_path LIKE '%{SPLIT_FOLDER}%'\n",
        "       OR relative_path LIKE '%_part_%'\n",
        "    \"\"\"\n",
        "    stream_check = session.sql(check_query).collect()\n",
        "    \n",
        "    print(f\"\\nüìä Current Status:\")\n",
        "    print(f\"   Split parts in stream (pending): {stream_check[0]['COUNT']}\")\n",
        "    \n",
        "    # Check parsed documents for split parts\n",
        "    parsed_query = f\"\"\"\n",
        "    SELECT COUNT(*) as count\n",
        "    FROM {DATABASE}.{SCHEMA}.parsed_documents\n",
        "    WHERE file_path LIKE '%{SPLIT_FOLDER}%'\n",
        "       OR file_name LIKE '%_part_%'\n",
        "    \"\"\"\n",
        "    parsed_check = session.sql(parsed_query).collect()\n",
        "    print(f\"   Split parts already parsed: {parsed_check[0]['COUNT']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Could not check pipeline status: {str(e)[:50]}\")\n",
        "\n",
        "print(\"\\nüïê Check back in a few minutes for processing to complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 10: Verify Splitting Results\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFICATION: LARGE DOCUMENT REGISTRY STATUS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check registry status\n",
        "status_query = f\"\"\"\n",
        "SELECT \n",
        "    split_status,\n",
        "    COUNT(*) as count\n",
        "FROM {LARGE_DOC_REGISTRY}\n",
        "GROUP BY split_status\n",
        "ORDER BY count DESC\n",
        "\"\"\"\n",
        "\n",
        "status_df = session.sql(status_query).to_pandas()\n",
        "print(\"\\nRegistry Status Summary:\")\n",
        "print(status_df.to_string(index=False))\n",
        "\n",
        "# Check split parts tracking\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFICATION: SPLIT PARTS UPLOADED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "parts_query = f\"\"\"\n",
        "SELECT \n",
        "    registry_id,\n",
        "    COUNT(*) as parts_count,\n",
        "    SUM(part_size) as total_size_bytes\n",
        "FROM {SPLIT_PARTS_TABLE}\n",
        "GROUP BY registry_id\n",
        "ORDER BY registry_id\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    parts_df = session.sql(parts_query).to_pandas()\n",
        "    if len(parts_df) > 0:\n",
        "        print(\"\\nSplit Parts by Document:\")\n",
        "        print(parts_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nNo split parts recorded yet.\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not query split parts: {str(e)[:50]}\")\n",
        "\n",
        "# Check files in external stage split folder\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFICATION: FILES IN EXTERNAL STAGE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    stage_query = f\"\"\"\n",
        "    SELECT relative_path, size, last_modified\n",
        "    FROM DIRECTORY({EXTERNAL_STAGE})\n",
        "    WHERE relative_path LIKE '%{SPLIT_FOLDER}%'\n",
        "       OR relative_path LIKE '%_part_%'\n",
        "    ORDER BY last_modified DESC\n",
        "    LIMIT 20\n",
        "    \"\"\"\n",
        "    stage_df = session.sql(stage_query).to_pandas()\n",
        "    \n",
        "    if len(stage_df) > 0:\n",
        "        print(f\"\\nSplit files in stage ({len(stage_df)} found):\")\n",
        "        print(stage_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nNo split files found in stage yet.\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not query stage directory: {str(e)[:50]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 11: Optional - Remove Original Large Files\n",
        "# ============================================\n",
        "\n",
        "# After splitting is complete and verified, you can optionally remove\n",
        "# the original large files from the external stage.\n",
        "# \n",
        "# ‚ö†Ô∏è WARNING: Only do this AFTER verifying split parts were uploaded successfully!\n",
        "\n",
        "REMOVE_ORIGINALS_ENABLED = False  # Set to True to enable removal\n",
        "\n",
        "if REMOVE_ORIGINALS_ENABLED:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CLEANUP: REMOVING ORIGINAL LARGE FILES\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Get documents that were successfully split\n",
        "    completed_query = f\"\"\"\n",
        "    SELECT registry_id, original_file_path, original_file_name, split_count\n",
        "    FROM {LARGE_DOC_REGISTRY}\n",
        "    WHERE split_status = 'split_uploaded'\n",
        "      AND split_count > 0\n",
        "    \"\"\"\n",
        "    \n",
        "    completed = session.sql(completed_query).collect()\n",
        "    \n",
        "    removed_count = 0\n",
        "    for record in completed:\n",
        "        registry_id = record['REGISTRY_ID']\n",
        "        file_path = record['ORIGINAL_FILE_PATH']\n",
        "        file_name = record['ORIGINAL_FILE_NAME']\n",
        "        \n",
        "        print(f\"\\n  Removing: {file_name}\")\n",
        "        \n",
        "        if remove_original_large_file(file_path):\n",
        "            removed_count += 1\n",
        "            print(f\"    ‚úÖ Removed successfully\")\n",
        "            \n",
        "            # Update registry status\n",
        "            update_query = f\"\"\"\n",
        "            UPDATE {LARGE_DOC_REGISTRY}\n",
        "            SET split_status = 'original_removed'\n",
        "            WHERE registry_id = '{registry_id}'\n",
        "            \"\"\"\n",
        "            session.sql(update_query).collect()\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è Could not remove\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Cleanup completed: {removed_count} original files removed\")\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è Original file removal is disabled.\")\n",
        "    print(\"   Set REMOVE_ORIGINALS_ENABLED = True to remove original large files\")\n",
        "    print(\"   after verifying split parts were uploaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splitting Complete! üéâ\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook has **split large documents** and uploaded the parts back to the **same external stage**. The event-driven pipeline handles everything else automatically!\n",
        "\n",
        "### What This Notebook Did:\n",
        "1. ‚úÖ Downloaded large documents from the external stage\n",
        "2. ‚úÖ Analyzed PDF page counts and sizes  \n",
        "3. ‚úÖ Split documents exceeding 400 pages into smaller parts\n",
        "4. ‚úÖ **Uploaded split parts back to the SAME external stage**\n",
        "\n",
        "### What Happens Automatically Next:\n",
        "5. üîÑ Directory table auto-refresh detects new files\n",
        "6. üîÑ Stream captures the new split parts\n",
        "7. üîÑ `parse_documents_task` parses each part with AI_PARSE_DOCUMENT\n",
        "8. üîÑ `classify_documents_task` classifies them\n",
        "9. üîÑ `extract_documents_task` extracts attributes\n",
        "10. üîÑ `chunk_documents_task` creates searchable chunks\n",
        "11. üîÑ Cortex Search Service updates (within 2 minutes)\n",
        "\n",
        "## No Manual Intervention Needed!\n",
        "\n",
        "The split parts are now in the external stage. The existing event-driven pipeline will:\n",
        "- Detect them automatically\n",
        "- Parse, classify, extract, and chunk them\n",
        "- Make them searchable via Cortex Search\n",
        "\n",
        "## Scheduling\n",
        "\n",
        "To run this notebook on a schedule in Snowflake Container Runtime:\n",
        "\n",
        "### Option 1: Snowflake Notebooks Scheduling\n",
        "Use the built-in scheduling feature in Snowflake Notebooks UI.\n",
        "\n",
        "### Option 2: Execute via Stored Procedure\n",
        "Create a stored procedure that wraps the notebook logic for task scheduling.\n",
        "\n",
        "## Monitoring\n",
        "\n",
        "Check the pipeline processing status:\n",
        "\n",
        "```sql\n",
        "-- Check large document registry status\n",
        "SELECT * FROM document_db.s3_documents.large_document_status;\n",
        "\n",
        "-- Check split parts in the stage\n",
        "SELECT relative_path, size, last_modified\n",
        "FROM DIRECTORY(@document_db.s3_documents.document_stage)\n",
        "WHERE relative_path LIKE '%split_documents%'\n",
        "   OR relative_path LIKE '%_part_%'\n",
        "ORDER BY last_modified DESC;\n",
        "\n",
        "-- Check if split parts have been parsed\n",
        "SELECT file_name, status, parse_timestamp\n",
        "FROM document_db.s3_documents.parsed_documents\n",
        "WHERE file_name LIKE '%_part_%'\n",
        "ORDER BY parse_timestamp DESC;\n",
        "```\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "If splitting fails:\n",
        "1. Check the `split_status` column in `large_document_registry` for error messages\n",
        "2. Review the `document_split_parts` table for tracking info\n",
        "3. Re-run the notebook - it only processes pending documents\n",
        "\n",
        "If split parts aren't being processed:\n",
        "1. Check the stream: `SELECT * FROM document_db.s3_documents.new_documents_stream`\n",
        "2. Check task status: `SHOW TASKS IN SCHEMA document_db.s3_documents`\n",
        "3. Ensure tasks are resumed: `ALTER TASK parse_documents_task RESUME`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
